//
// auto-generated by op2.py
//

void CalcCourantConstraint_omp4_kernel(
  int *map0,
  int map0size,
  double *arg3,
  double *data0,
  int dat0size,
  double *data1,
  int dat1size,
  double *data2,
  int dat2size,
  int *col_reord,
  int set_size1,
  int start,
  int end,
  int num_teams,
  int nthread){

  double arg3_l = *arg3;
  #pragma omp target teams num_teams(num_teams) thread_limit(nthread)  \
    map(to: m_qqc2_ompkernel)\
    map(to:col_reord[0:set_size1],map0[0:map0size],data0[0:dat0size],data1[0:dat1size],data2[0:dat2size])\
    map(tofrom: arg3_l) reduction(min:arg3_l)
  #pragma omp distribute parallel for schedule(static,1) reduction(min:arg3_l)
  for ( int e=start; e<end; e++ ){
    int n_op = col_reord[e];
    int map0idx;
    map0idx = map0[n_op + set_size1 * 0];

    //variable mapping
    const double *ss = &data0[1 * map0idx];
    const double *vdov = &data1[1 * map0idx];
    const double *arealg = &data2[1 * map0idx];
    double *dtcourant = &arg3_l;

    //inline function
    

      double dtf = ss[0] * ss[0];

      if ( vdov[0] < double(0.) ) {

          dtf = dtf
              + m_qqc2_ompkernel * arealg[0] * arealg[0]
              * vdov[0] * vdov[0] ;
      }

      dtf = sqrt(dtf) ;

      dtf = arealg[0] / dtf ;

      if (vdov[0] != double(0.)) {
          if ( dtf < (*dtcourant) ) {
          (*dtcourant) = dtf ;

          }
      }
    //end inline func
  }

  *arg3 = arg3_l;
}
